import sys

number_of_blocks = sys.argv[1] # number of blocks
number_of_segments = sys.argv[2] # number of segments

f = open("sssp.cuh", "w")

list_to_write = []

# the length of this should be = number_of_blocks = number_of_segments
list_of_kernels = []

# opening the file and reading in the kernel names 
f2 = open(sys.argv[3], "r")
while True:
    l = f2.readline()
    if l:
        list_of_kernels.append(l.strip())
    else:
        break
f2.close()
print(list_of_kernels)

# we will have to do a nam merge here to mimic the actual file
unique_kernel_starts = []
starting_kernel = ""
for i in range(0, len(list_of_kernels)):
    if starting_kernel == "":
        unique_kernel_starts.append(i)
        starting_kernel = list_of_kernels[i]
    elif starting_kernel != list_of_kernels[i]:
        unique_kernel_starts.append(i)
        starting_kernel =list_of_kernels[i]
unique_kernel_starts.append(len(list_of_kernels))

print(unique_kernel_starts)

# adding the header first
header_string = """/**
 * Heterogeneous implementation of the SSSP pull kernel.
 * This is generated by util/scheduler/scheduler/kernelgen/sssp_hetero.py.
 */

#ifndef SRC_KERNELS_HETEROGENEOUS__SSSP_CUH
#define SRC_KERNELS_HETEROGENEOUS__SSSP_CUH

#include <omp.h>
#include <vector>

#include "../kernel_types.cuh"
#include "../cpu/sssp.cuh"
#include "../gpu/sssp.cuh"
#include "../../cuda.cuh"
#include "../../graph.cuh"
#include "../../util.h"

constexpr int num_gpus = 1;\n"""

list_to_write.append(header_string)

GPU_butterfly_P2P_function_declaration = """
/** Forward decl. */
void gpu_butterfly_P2P(nid_t *seg_ranges, weight_t **cu_dists, 
        cudaStream_t *memcpy_streams);
"""

list_to_write.append(GPU_butterfly_P2P_function_declaration)

header_comment = """
/**
 * Runs SSSP kernel heterogeneously across the CPU and GPU. Synchronization 
 * occurs in serial. 
 * Configuration:
 *   - 1x 
 *   - 1x NVIDIA Quadro RTX 4000
 *
 * Parameters:
 *   - g         <- graph.
 *   - init_dist <- initial distance array.
 *   - ret_dist  <- pointer to the address of the return distance array.
 * Returns:
 *   Execution time in milliseconds.
 */
"""
list_to_write.append(header_comment)

# writing the function sssp_pull_heterogenous

function_header = ""
function_body = ""
function_tail = ""

function_header += """double sssp_pull_heterogeneous(const CSRWGraph &g, 
        const weight_t *init_dist, weight_t ** const ret_dist
) {\n"""

function_body += "    // Configuration.\n"

function_body += "   constexpr int num_blocks   = {}\n".format(number_of_blocks)
function_body += "   constexpr int num_segments = {}\n".format(number_of_segments)

function_body += """
    // Copy graph.
    nid_t *seg_ranges = compute_edge_ranges(g, num_segments); // OWN FUNCTION
    
    /// Block ranges to reduce irregular memory acceses."""

function_body += """    
    /// Block ranges to reduce irregular memory acceses.
    constexpr int gpu_blocks[] = {{0, {num}}};
    nid_t block_ranges[num_blocks * 2];\n""".format(num=number_of_blocks)

function_body += "\n"

# now we need to set up the block ranges
# TODO: Fix the i,i,i,i to the correct values: FIXED
# for us, number of blocks = number of segments | 1 block per segment
start = 0
end = 1
for i in range(0, int(number_of_blocks) * 2, 2):
    function_body += "    block_ranges[{}] = seg_ranges[{}]; // Block {} Start {}\n".format(i,unique_kernel_starts[start],i,unique_kernel_starts[start])
    function_body += "    block_ranges[{}] = seg_ranges[{}]; // Block {} End {} (excl.)\n".format(i + 1,unique_kernel_starts[end],i,unique_kernel_starts[end])
    start = end
    end += 1

function_body +="""
    /// Actual graphs on GPU memory.
    offset_t *cu_indices[num_blocks];
    wnode_t  *cu_neighbors[num_blocks];

    for (int gpu = 0; gpu < num_gpus; gpu++) {
        CUDA_ERRCHK(cudaSetDevice(gpu));
        for (int block = gpu_blocks[gpu]; block < gpu_blocks[gpu + 1];
                block++) 
            copy_subgraph_to_device(g,
                &cu_indices[block], &cu_neighbors[block],
                block_ranges[2 * block], block_ranges[2 * block + 1]);
    }\n"""

function_body += """
    // Initialize memcopy streams.
    // idx = from_gpu * num_gpus + to_gpu;
    cudaStream_t memcpy_streams[num_gpus * num_gpus];
    for (int from = 0; from < num_gpus; from++) {
        CUDA_ERRCHK(cudaSetDevice(from));
        for (int to = 0; to < num_gpus; to++)
            CUDA_ERRCHK(cudaStreamCreate(&memcpy_streams[from * num_gpus + to]));
    }\n"""

function_body += """    // Distance.
    size_t   dist_size = g.num_nodes * sizeof(weight_t);
    weight_t *dist     = nullptr; 

    /// CPU Distance.
    CUDA_ERRCHK(cudaMallocHost((void **) &dist, dist_size));
    #pragma omp parallel for
    for (int i = 0; i < g.num_nodes; i++)
        dist[i] = init_dist[i];

    /// GPU Distances.
    weight_t *cu_dists[num_gpus];
    for (int gpu = 0; gpu < num_gpus; gpu++) {        
        CUDA_ERRCHK(cudaSetDevice(gpu));
        CUDA_ERRCHK(cudaMalloc((void **) &cu_dists[gpu], dist_size));
        CUDA_ERRCHK(cudaMemcpyAsync(cu_dists[gpu], dist, dist_size,
            cudaMemcpyHostToDevice, memcpy_streams[gpu * num_gpus]));
    }
    for (int gpu = 0; gpu < num_gpus; gpu++) {
        CUDA_ERRCHK(cudaStreamSynchronize(memcpy_streams[gpu * num_gpus]));
    }\n"""

function_body += """
    // Update counter.
    nid_t updated     = 1;
    nid_t cpu_updated = 0;
    nid_t *cu_updateds[num_gpus];
    for (int gpu = 0; gpu < num_gpus; gpu++) {
        CUDA_ERRCHK(cudaSetDevice(gpu));
        CUDA_ERRCHK(cudaMalloc((void **) &cu_updateds[gpu], 
                sizeof(nid_t)));
    }

    // Create compute streams and markers.
    cudaStream_t compute_streams[num_blocks]; // Streams for compute.
    cudaEvent_t  compute_markers[num_blocks]; // Compute complete indicators.
    for (int gpu = 0; gpu < num_gpus; gpu++) {
        CUDA_ERRCHK(cudaSetDevice(gpu));
        for (int b = gpu_blocks[gpu]; b < gpu_blocks[gpu + 1]; b++) {
            CUDA_ERRCHK(cudaStreamCreate(&compute_streams[b]));
            CUDA_ERRCHK(cudaEventCreate(&compute_markers[b]));
        }
    }\n"""

function_body += """
    // Get init vertex.
    // TODO: add this as a parameter.
    nid_t start;
    for (nid_t i = 0; i < g.num_nodes; i++)
        if (init_dist[i] != INF_WEIGHT) start = i;

    // Start kernel!
    Timer timer; timer.Start();
    int epochs = 0;

    /*
    // Push for the first iteration.
    // TODO: implement push for more than one epoch. Requires parallel queue.
    for (wnode_t nei : g.get_neighbors(start)) {
        if (nei.v == start) continue;

        dist[nei.v] = nei.w;       
        for (int gpu = 0; gpu < num_gpus; gpu++) {
            CUDA_ERRCHK(cudaSetDevice(gpu));
            CUDA_ERRCHK(cudaMemcpyAsync(
                cu_dists[gpu] + nei.v, dist + nei.v,
                sizeof(weight_t), cudaMemcpyHostToDevice));
        }
    }
    epochs++;
    */

    while (updated != 0) {
        // Reset update counters.
        updated = cpu_updated = 0;          
        for (int gpu = 0; gpu < num_gpus; gpu++) {
            CUDA_ERRCHK(cudaSetDevice(gpu));
            CUDA_ERRCHK(cudaMemsetAsync(cu_updateds[gpu], 0, 
                    sizeof(nid_t)));
        }

        // Launch GPU epoch kernels.
        // Implicit CUDA device synchronize at the start of kernels.\n"""

function_body += "        CUDA_ERRCHK(cudaSetDevice(0));\n"
# can also have it from 0 -> number_of_segments because for us the values will be the same
# TODO: Complete the thing below
block_range_count = 0
for i in range(0, int(number_of_blocks)):
    kernel_name = ""
    first_num = ""
    second_num = ""

    if "SSSP GPU block-red" in list_of_kernels[unique_kernel_starts[i]]:
       kernel_name = "epoch_sssp_pull_gpu_block_red"
       l = list_of_kernels[unique_kernel_starts[i]].split()
       first_num = l[-2]
       second_num = l[-1]
    elif "SSSP GPU warp-red" in list_of_kernels[unique_kernel_starts[i]]:
        kernel_name = "epoch_sssp_pull_gpu_warp_red"
        l = list_of_kernels[unique_kernel_starts[i]].split()
        first_num = l[-2]
        second_num = l[-1]
    elif "SSSP CPU one-to-one" in list_of_kernels[unique_kernel_starts[i]]:
        kernel_name = "epoch_sssp_pull_cpu_one_to_one"
        l = list_of_kernels[unique_kernel_starts[i]].split()
        first_num = l[-2]
        second_num = l[-1]
    elif "SSSP GPU one-to-one" in list_of_kernels[unique_kernel_starts[i]]:
        kernel_name = "epoch_sssp_pull_gpu_one_to_one"
        l = list_of_kernels[unique_kernel_starts[i]].split()
        first_num = l[-2]
        second_num = l[-1]

    function_body += """{}<<<{}, {}, 0, compute_streams[{}]>>>(
                cu_indices[{}], cu_neighbors[{}],
                block_ranges[{}], block_ranges[{}],
                cu_dists[{}], cu_updateds[{}]);\n""".format(kernel_name, first_num, second_num, i, i, i, block_range_count, block_range_count + 1, 0, 0)
    
    function_body += "        CUDA_ERRCHK(cudaEventRecord(compute_markers[{}], compute_streams[{}]));\n".format(i, i)
    block_range_count += 2

function_body += """
        // Launch CPU epoch kernels.
                

        // Sync compute streams.
        for (int b = 0; b < num_blocks; b++)
            CUDA_ERRCHK(cudaEventSynchronize(compute_markers[b]));

        // Synchronize updates.
        nid_t gpu_updateds[num_gpus];
        for (int gpu = 0; gpu < num_gpus; gpu++) {
            CUDA_ERRCHK(cudaSetDevice(gpu));
            CUDA_ERRCHK(cudaMemcpyAsync(
                    &gpu_updateds[gpu], cu_updateds[gpu],  sizeof(nid_t), 
                    cudaMemcpyDeviceToHost, memcpy_streams[gpu * num_gpus + gpu]));
        }
        updated += cpu_updated;

        for (int gpu = 0; gpu < num_gpus; gpu++) {
            CUDA_ERRCHK(cudaSetDevice(gpu));
            CUDA_ERRCHK(cudaStreamSynchronize(memcpy_streams[gpu * num_gpus + gpu]));
            updated += gpu_updateds[gpu];
        }

        // Only update GPU distances if another epoch will be run.
        if (updated != 0) {
            // Copy CPU distances to all GPUs.
            

            // Copy GPU distances peer-to-peer.
            // Not implmented if INTERLEAVE=true.
            gpu_butterfly_P2P(seg_ranges, cu_dists, memcpy_streams); 

            // Synchronize HtoD async calls.
            
        }

        

        
        epochs++;
    }"""

function_body += """    // Copy GPU distances back to host.
    CUDA_ERRCHK(cudaSetDevice(0))
    CUDA_ERRCHK(cudaMemcpyAsync(
        dist + seg_ranges[0], cu_dists[0] + seg_ranges[0],
        (seg_ranges[{num}] - seg_ranges[0]) * sizeof(weight_t), 
        cudaMemcpyDeviceToHost, memcpy_streams[0 * num_gpus + 0]));
    // Wait for memops to complete.
    for (int gpu = 0; gpu < num_gpus; gpu++) {{
        CUDA_ERRCHK(cudaSetDevice(gpu));
        CUDA_ERRCHK(cudaDeviceSynchronize());
    }}
    
    timer.Stop();\n""".format(num = number_of_segments)

function_body += """
    // Copy output.
    *ret_dist = new weight_t[g.num_nodes];
    #pragma omp parallel for
    for (int i = 0; i < g.num_nodes; i++)
        (*ret_dist)[i] = dist[i];

    // Free streams.
    for (int gpu = 0; gpu < num_gpus; gpu++) {
        CUDA_ERRCHK(cudaSetDevice(gpu));
        for (int b = gpu_blocks[gpu]; b < gpu_blocks[gpu + 1]; b++) {
            CUDA_ERRCHK(cudaStreamDestroy(compute_streams[b]));
            CUDA_ERRCHK(cudaEventDestroy(compute_markers[b]));
        }

        for (int to = 0; to < num_gpus; to++)
            CUDA_ERRCHK(cudaStreamDestroy(memcpy_streams[gpu * num_gpus + to]));
    }

    // Free memory.
    for (int gpu = 0; gpu < num_gpus; gpu++) {
        CUDA_ERRCHK(cudaSetDevice(gpu));
        CUDA_ERRCHK(cudaFree(cu_updateds[gpu]));
        CUDA_ERRCHK(cudaFree(cu_dists[gpu]));
        
        for (int block = gpu_blocks[gpu]; block < gpu_blocks[gpu + 1];
                block++
        ) {
            CUDA_ERRCHK(cudaFree(cu_indices[block]));
            CUDA_ERRCHK(cudaFree(cu_neighbors[block]));
        }
    }
    CUDA_ERRCHK(cudaFreeHost(dist));
    delete[] seg_ranges;

    return timer.Millisecs();
}\n"""

function_body += """
/**
 * Enable peer access between all compatible GPUs.
 */
void enable_all_peer_access() {
    int can_access_peer;
    for (int from = 0; from < num_gpus; from++) {
        CUDA_ERRCHK(cudaSetDevice(from));

        for (int to = 0; to < num_gpus; to++) {
            if (from == to) continue;

            CUDA_ERRCHK(cudaDeviceCanAccessPeer(&can_access_peer, from, to));
            if(can_access_peer) {
                CUDA_ERRCHK(cudaDeviceEnablePeerAccess(to, 0));
                std::cout << from << " " << to << " yes" << std::endl;
            } else {
                std::cout << from << " " << to << " no" << std::endl;
            }
        }
    }
}

/**
 * Butterfly GPU P2P transfer.
 */
void gpu_butterfly_P2P(nid_t *seg_ranges, weight_t **cu_dists, 
    cudaStream_t *memcpy_streams
) {
    
}

#endif // SRC_KERNELS_HETEROGENEOUS__SSSP_CUH\n"""

# writing everything in the list
list_to_write.append(function_header)
list_to_write.append(function_body)
list_to_write.append(function_tail)

# writing down all the strings to the file
f.write("".join(list_to_write))

# closing the file
f.close()